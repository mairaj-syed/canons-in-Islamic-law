{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: IMPORTING QARAFI'S *DHAKHIRA* FROM [OPENITI](https://github.com/OpenITI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib.request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the url below from [OpenITI's catalog of digitized Islamic sources](https://kitab-corpus-metadata.azurewebsites.net/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_url=\"https://raw.githubusercontent.com/OpenITI/0700AH/master/data/0684ShihabDinQarafi/0684ShihabDinQarafi.Dhakhira/0684ShihabDinQarafi.Dhakhira.JK003500-ara1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_text=urllib.request.urlopen(book_url).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_text=book_text.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2: PREPARING TEXT FOR SEMANTIC SIMILARITY PROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2.1: LOCATING THE TESTIMONY CHAPTER (KITAB AL-SHAHADAT) IN THE *DHAKIRA*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use regular expressions to find all instances of chapter titles in the text of the Dhakhira. The two cells below define the regular expression and then use it create a list of all chapter titles in the text. Since we will ultimately partition only chapter on testimony, we need to know how the beginning and end of that chapter is specifically demarcated in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chaptertitles=re.compile('#.+\\|.+\\\\(.كتاب.+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titleinstances=chaptertitles.findall(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleinstances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titleinstances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleinstances[31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Removing Artificial Line Breaks and Partitioning Dhakhira by Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OpenITI, for readability partitioned lines artificially and indicated that by two tildes \"~~ \". In order to partition into paragraph we need to replace all new lines followed by two tildes with a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_text_prep_for_paras=book_text.replace('\\n~~',' ~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book_text_para=re.split('\\n',book_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book_text_para variable now contains the dhakhira divided into paragraphs in list form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locating the Paragaphs that Belong to the Testimony Chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below locates the paragraph number which demarcates the beginning of the testimony chapter, by giving as an input the title of the testimony chapter: (# | كتاب الشهادات)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text_para.index(titleinstances[31])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text_para[69069]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_text_para.index(kitabtitleinstances[32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Using the index command above, we have discovered that the paragraphs that belong to the testimony (Shahada) chapter comprise of paragraphs 69069 to 72434. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kitab_shahada_text_para=book_text_para[69069:72435]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command above puts only the paragraphs that constitute the testimony chapter into the variable kitab_shahada_text_para. The command below takes the paragraphs that make up the testimony chapter and sews them back together into a one text contained in one variable that is NOT a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kitab_shahada_text=\" \".join(kitab_shahada_text_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2.2 CLEANING THE TESTIMONY CHAPTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important steps in getting semantic similarity scores is to remove all characters, such as latin characters, punction marks, etc. from the testimony chapter - basically everything that is not an Arabic letter. One way to do that is to count all instances of all unique characters that make up the testimony chapter. Then create a set that contains the characters you want to remove, and then delete them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below takes a string (or text) as an input, and returns a list of all the unique characters it contains along with the number of times it occurs in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_kitab_shahada_text=Counter(kitab_shahada_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_kitab_shahada_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are interested in not necessarily the frequency of the occurence of each character in the text, but just the unique character themselves. But, in order to do that we have to convert it into a list format. The two commands below do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_and_count_kitab_jinaya_text=dict(letters_kitab_shahada_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters_kitab_jinaya_text=list(letters_and_count_kitab_jinaya_text.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I manually generated the list below. Based on the output of the \"Counter\" command, I identified all non-Arabic character that occur in the testimony chapter, and put them into a list. I named the list 'cancel'. I'll use this list to go through the testimony chapter and delete each of these characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cancel=['#','|','(',\")\",':','~','3','@','Q','B','E','P','a','g','e','V','2','0','5','m','s','4','7','^','6','&','8','%','9','-','.','_','$','،','\\'','1','!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command below, I move the contents of the testimony chapter into a new variable. Why? In case my deletion code messes up. I have an untouched copy I can return to. I'll do my deletions on the new copy -- called kitab_shahada_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kitab_shahada_text2=kitab_shahada_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below uses a loop to go through and delete each character I placed in the list named 'cancel'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in cancel:\n",
    "    kitab_shahada_text2=kitab_shahada_text2.replace(i,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to check to make sure I deleted all non-Arabic characters. So I run the Counter command on the variable kitab_shahada_text2. And sure enough, I was successful. (Not on the first try of course!! LOL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(kitab_shahada_text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2.3 CREATING THE 20-GRAMS FROM THE TESTIMONY CHAPTER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a command called 'ngrams' which is found in a popular natural language processing library called NLTK. But, in order to create 20-grams using the ngrams command, I have to feed it text in which the words that make up the testimony chapter is converted into a list in which each word is an element of the list. The command below does that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_kitab_shahada_text=kitab_shahada_text2.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've probably seen this 'len' command before up above. It is a very useful command that will return the number of elements in a list, if it is applied to a list, and if it is applied to a string (or text) it will return the number of characters that makeu up the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_kitab_shahada_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command below downloads the ngrams command from the nltk.util library. Python does this automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to feed the ngrams command our list of words that make up the testimony chapter. We also specify that we want it to create 20-grams, as opposed to 7, or 9, or whatever number you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twentygrams_kitab_shahada_text=ngrams(words_kitab_shahada_text,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the ngrams command, which I stored in the 'twentygrams_kitab_shahada_text' variable is in a format that only other nltk commands can understand. But, we need to create 20-grams that out Google semantic search service can take as in input -- which are regular phrases. So we need to convert the output into a list. Each element of the list will be 20 words long and in fact the 20 words themselves will be elements of their own list. So we wil in fact have a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twentygrams_kitab_shahada=list(twentygrams_kitab_shahada_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't feed GSSS a list of words. We need to recombine them into phrases that consist of 20 words in length. The code below does just that by using a loop to go through the main list of 20-gram phrases making up the twentygrams_kitab_shahada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined_twentygrams_kitab_shahada=[]\n",
    "for i in range(0,len(twentygrams_kitab_shahada)):\n",
    "    fragment=' '.join(twentygrams_kitab_shahada[i])\n",
    "    joined_twentygrams_kitab_shahada.append(fragment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(joined_twentygrams_kitab_shahada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_twentygrams_kitab_shahada[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've made a lot of progress. From time to time is good to save that progress in the form of an external file. So I saved the twenty-grams to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('20gram_kitab_shahada_fragments.csv','w',encoding='utf-8',newline='\\n') as fragmentfile:\n",
    "    wr=csv.writer(fragmentfile,dialect='excel')\n",
    "    for fragment in joined_twentygrams_kitab_shahada:\n",
    "        wr.writerow([fragment])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 3: SENDING FRAGMENTS AND RECEIVING SIMILARITY SCORES FROM GSSS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now, you don't need anything other than having installed python -- jupyter notebook on your laptop. But, in order to proceed further you need to have done many things. As noted in my blog, the GSSS is an experimental service provided by Google cloud. That means, it is not commercially available yet. So, you need to get approved to use it. You can apply for the service [here](https://events.withgoogle.com/ai-workshop/registrations/my-rsvp/confirm-account/). Then you need to follow their detailed instructions in order get the service set up. Unfortunately, those detailed instructions are only provided if you get accepted to the experimental program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below downloads the specific command Google provided in order to send their cloud service information from within Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import googleapiclient.discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is a template of code that GSSS provides correctly format the \"input text\" and the \"candidate texts\" and the security crediantials needed to get access to the cloud service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_candidates(input_text,\n",
    "                  candidate_text_list,\n",
    "                  project='semantic-similarity-for-text',\n",
    "                  model='universal_encoder',\n",
    "                  version=None):\n",
    " \"\"\"Rank candidates against given input.\n",
    "\n",
    " Args:\n",
    "   input_text: input text\n",
    "   candidate_text_list: list of candidate text to rank.\n",
    "   project: cloud ml project id\n",
    "   model: model name\n",
    "   version: model version\n",
    "\n",
    " Returns:\n",
    "    list of tuples of (candidate text, score) sorted by score in reverse order.\n",
    "\n",
    " Raises:\n",
    "   RuntimeError: if API call failed.\n",
    " \"\"\"\n",
    " service = googleapiclient.discovery.build('ml', 'v1')\n",
    " name = 'projects/{}/models/{}'.format(project, model)\n",
    " if version is not None:\n",
    "   name += '/versions/{}'.format(version)\n",
    "\n",
    " instances = []\n",
    " for r in candidate_text_list:\n",
    "   instances.append({'Input': input_text, 'Candidate': r})\n",
    "\n",
    " response = service.projects().predict(\n",
    "     name=name,\n",
    "     body={\n",
    "         'instances': instances,\n",
    "     }).execute()\n",
    "\n",
    " if 'error' in response:\n",
    "   raise RuntimeError(response['error'])\n",
    "\n",
    " return sorted(\n",
    "     zip(candidate_text_list,\n",
    "         [p['similarity_score'][0] for p in response['predictions']]),\n",
    "     reverse=True,\n",
    "     key=lambda p: p[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above takes as an input, at a minimum a pair of texts -- one is the input text, which in our cases is the text of the canon we are searching for, and the other is a list of 20-gram fragments for which we want a semantic score. GSSS says, don't send a list of more than 1000 candidate texts. But, we have 41,698 20-grams. How do we handle this? We use a loop to send 1000 at a time until the list is finished.\n",
    "\n",
    "Here's the problem: when I tried to send a 1000 at a time, the service sometimes timed out. With some experimentaiton, I determined 200 is optimal. The next two cells contain the code that loops through the 20-grams list, sending GSSS the information they want, and receiving and recording their response.\n",
    "\n",
    "Because the service would time out, I had make a simple way of keeping track of what scores I successfully recieved and recorded versus the ones that I still needed scores for.\n",
    "\n",
    "Note: before you execute the two cells below, you need to log in to GSSS's cloud service so you have the right security credentials to get a response back from GSSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outputmid=[]\n",
    "#variable that will record GSSS's response, before it is saved to a file external to python.\n",
    "\n",
    "input_text='البينة على المدعي واليمين على من أنكر'\n",
    "#the canon we are searching for\n",
    "\n",
    "initial=0\n",
    "#the intial list number we begin with\n",
    "\n",
    "interval=200\n",
    "#how many fragments we want to send to GSSS at one time.\n",
    "\n",
    "final=400\n",
    "#the position of the last 20-gram fragment you want to stop at. Sometimes this is useful, \n",
    "#if you don't want all scores all at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(initial,final,interval):\n",
    "    \n",
    "    print('current iteration is: ' + str(i))\n",
    "    # this helps you keep track of the last set of fragments you have sent to GSSS\n",
    "    \n",
    "    outputmid=rank_candidates(input_text,joined_twentygrams_kitab_shahada[i:i+interval])\n",
    "    # this is the core line of code. It is what sends the canon and the list of 20-grams to GSSS.\n",
    "    \n",
    "    with open('bayyina_shahada_similarity_scores.csv','a+',encoding='utf-8',newline='\\n') as scorefile:\n",
    "        wr=csv.writer(scorefile,dialect='excel')\n",
    "        wr.writerows(outputmid)\n",
    "    #This takes the output recieved from GSSS and saves it to an external file.\n",
    "        \n",
    "    print('just added '+str(len(outputmid))+' scores!')\n",
    "    #This tells you that you were successfully able to save x number of scores to the file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}